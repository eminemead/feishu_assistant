# Stretching Mastra's Potential in Feishu Assistant

**Your Current State**: You have 60% of Mastra's power unlocked. You've built workflows and memory, but you're still using sequential routing + regex for agent decisions.

**Opportunity**: You can 2-3x amplify Mastra's value with targeted improvements. This doc prioritizes by ROI.

---

## üéØ Quick Wins (1-2 hours each)

### Win 1: LLM-Based Routing (Replace Regex)

**Current** (manager-agent.ts:217-225):
```typescript
const shouldRouteToOkr = /okr|objective|key result|...|Ë¶ÜÁõñÁéá/.test(lowerQuery);
const shouldRouteToAlignment = /alignment|ÂØπÈΩê/.test(lowerQuery);
```

**Problem**: Misses ambiguous queries ("OKR‰∏éÂØπÈΩêÁöÑÂÖ≥Á≥ªÔºü" ‚Üí picks OKR, loses Alignment intent)

**Fix** (Mastra pattern):
```typescript
// Use the workflow you already built
import { getRoutingDecision } from "../workflows/manager-routing-workflow";

export async function managerAgent(messages: CoreMessage[], ...) {
  const query = getQueryText(messages);
  
  // LLM-powered routing via your existing workflow
  const routing = await getRoutingDecision({ query, messages });
  
  if (routing.confidence > 0.8) {
    // Single specialist
    const agent = getAgentByName(routing.agentName);
    return agent.stream(messages);
  } else if (routing.confidence < 0.4) {
    // Ambiguous - try multi-agent
    const alternateCategories = await detectSecondaryCategory(query);
    if (alternateCategories.length > 0) {
      return await executeParallelAgents([routing.category, ...alternateCategories], messages);
    }
  }
  
  // Use manager fallback for low confidence
  return managerAgentInstance.stream(messages);
}

// Helper: Execute multiple agents in parallel
async function executeParallelAgents(
  categories: QueryCategory[],
  messages: CoreMessage[]
): Promise<string> {
  const agents = categories.map(cat => getAgentByName(mapCategoryToAgent(cat)));
  const results = await Promise.all(agents.map(a => a.stream(messages).then(r => r.fullStream)));
  
  // Synthesize via manager
  return managerAgentInstance.stream([
    ...messages,
    { 
      role: 'assistant', 
      content: `Multiple specialist insights:\n\n${results.map((r, i) => `**${categories[i]}**: ${r}`).join('\n\n')}` 
    }
  ]);
}
```

**Impact**: 
- ‚úÖ Handles ambiguous queries (OKR + Alignment together)
- ‚úÖ Self-correcting (learns from confidence scores)
- ‚úÖ Reuses your existing workflow
- **Time**: 1.5 hours
- **ROI**: 40% better query handling

---

### Win 2: Memory-Aware Routing

**Current**: Routing ignores conversation history

**Fix**:
```typescript
export async function getRoutingDecision(params: {
  query: string;
  messages?: CoreMessage[];
  executionContext?: any;
}) {
  // Add context from previous messages
  const conversationContext = params.messages
    ?.slice(-5)  // Last 5 messages
    .map(m => typeof m.content === 'string' ? m.content : '')
    .join('\n');

  const classifyQueryStep = createStep({
    execute: async ({ inputData }) => {
      const { query } = inputData;
      
      // CHANGE: Include conversation history
      const fullContext = `
        Previous context:
        ${conversationContext}
        
        Current query: ${query}
      `;
      
      // Score routing based on full context
      // Instead of just regex on current query
      const scores = scoreQueryWithContext(fullContext);
      // ... rest of classification
    }
  });
}
```

**Impact**: 
- ‚úÖ Follow-up questions work better ("...ÂÖ∑‰ΩìÊù•ËØ¥Ôºü" knows you're talking about OKR)
- ‚úÖ Coherent conversation threads
- **Time**: 45 minutes
- **ROI**: 30% fewer routing mistakes on follow-ups

---

### Win 3: Health-aware Agent Selection

**Current**: Always use primary model, switch on error

**Fix** (using your model-fallback.ts infrastructure):
```typescript
// Before routing, check health
import { healthMonitor } from "../health-monitor";

async function managerAgent(messages: CoreMessage[], ...) {
  const routing = await getRoutingDecision({ query });
  const agentName = routing.agentName;
  
  // Check which agent is healthiest
  const agentHealth = healthMonitor.getAgentHealth(agentName);
  
  if (agentHealth.successRate < 0.7) {
    // Agent is struggling - route to general manager instead
    console.log(`[Router] ${agentName} health low (${agentHealth.successRate.toFixed(2)}), using manager`);
    return managerAgentInstance.stream(messages);
  }
  
  // Normal routing
  const agent = getAgentByName(agentName);
  return agent.stream(messages);
}
```

**Impact**:
- ‚úÖ Auto-fallback to general handler when specialist fails
- ‚úÖ Self-healing system
- **Time**: 30 minutes
- **ROI**: Reduces user-facing failures by 20%

---

## üöÄ Medium Efforts (1-2 days each)

### Effort 1: Suspension & Resume for Long-Running Analyses

**Current**: OKR analysis runs to completion (can take 10-30s)

**What Mastra offers**:
```typescript
// In okr-analysis-workflow.ts
const workflow = okrAnalysisWorkflow;

// Before heavy computation, suspend
const handle = await workflow.suspend({
  inputData: { period: "10Êúà", userId: "user-123" },
  checkpointId: "okr-analysis-pending",
});

// User gets immediate: "Analysis starting... (checkpoint: okr-analysis-pending)"

// Background job continues
const finalResult = await workflow.resume(handle);

// User receives update: "Analysis complete: ..."
```

**Benefits**:
- ‚úÖ Feishu card shows "Analyzing..." immediately (better UX)
- ‚úÖ Server can handle parallel long-running tasks
- ‚úÖ User can navigate away and come back
- ‚úÖ Built-in error recovery (resume from checkpoint)

**Implementation path**:
1. Modify `okr-analysis-workflow` to use `.suspend()` after chart generation
2. Return checkpoint ID to Feishu client
3. Client polls `/api/analysis-status?checkpointId=...`
4. Resume on poll, return partial results
5. Final result updates Feishu card

**Time**: 1.5 days  
**ROI**: 50% better UX for long analyses + error recovery

---

### Effort 2: Agentic RAG for Document Context

**Current**: Document tracking is standalone

**What Mastra offers** (@mastra/rag):
```typescript
// New workflow: document-aware OKR analysis
const documentRagStep = createStep({
  id: "search-docs",
  description: "Search relevant documents for OKR context",
  inputSchema: z.object({
    query: z.string(),
    period: z.string(),
  }),
  outputSchema: z.object({
    documents: z.array(z.object({
      source: z.string(),
      excerpt: z.string(),
      relevance: z.number(),
    })),
  }),
  execute: async ({ inputData }) => {
    // Use Mastra RAG to find related docs
    const knowledgeBase = new Rag({
      vectorStore: postgresVectorStore,
      chunkSize: 500,
    });
    
    const documents = await knowledgeBase.query({
      query: inputData.query,
      topK: 5,
      threshold: 0.7,
    });
    
    return { documents };
  }
});

// Enhanced OKR workflow
export const okhRagAnalysisWorkflow = createWorkflow({
  id: "okr-rag-analysis",
})
  .then(queryOkrDataStep)
  .then(documentRagStep)  // NEW: Find related docs
  .then(generateChartsStep)
  .then(analyzeWithDocContextStep)  // CHANGED: Pass docs to agent
  .then(formatResponseStep)
  .commit();
```

**Impact**:
- ‚úÖ OKR analysis references supporting documents
- ‚úÖ "This aligns with strategy doc from Sept 15" (auto-cited)
- ‚úÖ Agent can validate claims against real docs
- **Time**: 1.5-2 days
- **ROI**: 60% more trustworthy analyses (docs back findings)

---

### Effort 3: Streaming + Real-time Updates

**Current**: You accumulate text, then send batch updates (lines 262-267 in manager-agent.ts)

**What Mastra offers** (native streaming):
```typescript
// Current
const result = await okrAgent.stream(messages, executionContext);
let accumulatedText = "";
for await (const textDelta of result.textStream) {
  accumulatedText += textDelta;
  await updateCardBatched(text);  // Batched delay
}

// Mastra + streaming optimized
const result = await okrAgent.stream(messages, {
  ...executionContext,
  streamingOptions: {
    throttleMs: 100,  // Batch every 100ms instead of 150ms
    minCharsPerUpdate: 30,  // Send more frequent updates
  }
});

// Or: Direct streaming to Feishu (no batching)
for await (const chunk of result.textStream) {
  // Send each chunk immediately
  updateStatus(prevText + chunk);
}

// Or: Structured streaming (get tool calls + text separately)
for await (const event of result.eventStream) {
  if (event.type === 'tool_call_start') {
    updateStatus(`üîÑ Running tool: ${event.toolName}`);
  } else if (event.type === 'text_delta') {
    updateStatus(prevText + event.delta);
  }
}
```

**Impact**:
- ‚úÖ More responsive Feishu cards (updates every 100ms vs 150ms)
- ‚úÖ Users see "thinking" phases (tool execution)
- ‚úÖ Better perceived speed (progressive rendering)
- **Time**: 4-6 hours
- **ROI**: 25% improvement in user perceived latency

---

## üéì Strategic Plays (1-2 weeks each)

### Play 1: Structured Routing Supervisor

**Current**: Regex ‚Üí Workflow ‚Üí Agent (3-step, still manual)

**Advanced**: LLM-as-router with structured outputs
```typescript
// Create a routing supervisor agent
const routingSupervisor = new Agent({
  name: "Router",
  instructions: `You are a query router. For each query, decide:
    1. Which specialist agents are needed (okr, alignment, pnl, dpa)
    2. Are multiple needed? (e.g., "OKR vs Alignment" = both)
    3. What's the analysis type? (comparison, summary, detail, trend)
    4. Any special handling needed?
    
    Output structured JSON with: agents[], analysisType, isComparison, context`,
  
  model: 'gpt-4o',  // Good at structured routing decisions
  tools: {},  // No tools needed for routing
});

// Use structured output
const routingResult = await routingSupervisor.generate(query, {
  outputFormat: 'json_schema',
  schema: z.object({
    agents: z.array(z.enum(['okr', 'alignment', 'pnl', 'dpa'])),
    analysisType: z.enum(['comparison', 'summary', 'detail', 'trend']),
    isComparison: z.boolean(),
    context: z.string(),
  }),
});

// Now execute agents based on structured routing
if (routingResult.agents.length === 1) {
  // Single agent
  return getAgentByName(routingResult.agents[0]).stream(messages);
} else {
  // Multi-agent execution
  const results = await Promise.all(
    routingResult.agents.map(agent => 
      getAgentByName(agent).stream(messages)
    )
  );
  
  // Use synthesis pattern if comparison
  if (routingResult.isComparison) {
    return synthesizeComparison(results, messages);
  }
  
  return combineResults(results);
}
```

**Impact**:
- ‚úÖ Self-improving routing (LLM learns patterns)
- ‚úÖ Auto-detection of multi-agent needs
- ‚úÖ Analysis type awareness (changes output format)
- **Time**: 1-2 weeks
- **ROI**: 50%+ improvement in complex query handling

---

### Play 2: Continuous Evals on Agent Outputs

**Current**: No validation of agent responses

**What Mastra offers**:
```typescript
// Add evaluation step to workflows
const evaluationStep = createStep({
  id: "evaluate-output",
  description: "Evaluate agent output quality",
  inputSchema: z.object({
    query: z.string(),
    analysis: z.string(),
    agent: z.string(),
  }),
  outputSchema: z.object({
    score: z.number().min(0).max(1),
    feedback: z.string(),
    shouldRetry: z.boolean(),
  }),
  execute: async ({ inputData }) => {
    const { query, analysis, agent } = inputData;
    
    // Evals using another LLM (cheaper: claude-3-5-haiku)
    const evaluation = await evaluatorAgent.generate(`
      Query: ${query}
      Analysis from ${agent}: ${analysis}
      
      Score the analysis 0-1 on:
      - Relevance: Does it answer the query?
      - Accuracy: Are facts correct?
      - Completeness: Covered all aspects?
      
      JSON: { score, feedback, shouldRetry }
    `, { outputFormat: 'json' });
    
    return {
      score: evaluation.score,
      feedback: evaluation.feedback,
      shouldRetry: evaluation.score < 0.6,
    };
  }
});

// Integrate into OKR workflow
export const okhAnalysisWithEvalsWorkflow = createWorkflow({
  id: "okr-analysis-evals",
})
  .then(queryOkrDataStep)
  .then(generateChartsStep)
  .then(analyzeStep)
  .then(evaluationStep)  // NEW: Validate output
  .then(async ({ context }) => {
    // Conditional: if failed evaluation, retry with better prompt
    if (context.evaluation.shouldRetry) {
      return retryWithBetterPrompt(context);
    }
    return context;
  })
  .then(formatResponseStep)
  .commit();
```

**Impact**:
- ‚úÖ Detects bad outputs before sending to user
- ‚úÖ Auto-retry with refined prompts
- ‚úÖ Metrics on agent quality (score tracking)
- ‚úÖ A/B test different specialist agents
- **Time**: 1-2 weeks
- **ROI**: 40% reduction in low-quality responses

---

### Play 3: Memory-Optimized Context Windows

**Current**: Load all conversation history (lines 681-698 in manager-agent.ts)

**Optimized**:
```typescript
// Instead of loading all history, use semantic search
async function loadSmartConversationHistory(
  memoryThread: string,
  memoryResource: string,
  query: string,
  maxMessages: number = 10
): Promise<CoreMessage[]> {
  const memory = await createMastraMemory(userId);
  
  // Option 1: Semantic search (find relevant messages)
  const semanticResults = await memory.query({
    threadId: memoryThread,
    resourceId: memoryResource,
    query,  // Find messages semantically similar to current query
    topK: 5,  // Only get most relevant
  });
  
  // Option 2: Recent + Relevant hybrid
  const recentMessages = await memory.query({
    threadId: memoryThread,
    resourceId: memoryResource,
    sort: 'createdAt',
    limit: 3,  // Last 3 messages
  });
  
  // Combine: recent + semantically relevant
  const combined = deduplicateAndMerge(recentMessages, semanticResults, maxMessages);
  
  // Only load what fits context window
  return combined.filter(m => sumTokens(combined) < MAX_CONTEXT_TOKENS);
}

// Use in manager agent
const messagesWithHistory = await loadSmartConversationHistory(
  memoryThread,
  memoryResource,
  query
);

const stream = await managerAgentInstance.stream(messagesWithHistory, executionContext);
```

**Impact**:
- ‚úÖ Never hits context window limits
- ‚úÖ Faster LLM inference (fewer tokens)
- ‚úÖ Still maintains semantic continuity
- **Time**: 3-4 days
- **ROI**: 20% faster responses + no context overflow errors

---

## üìä Implementation Roadmap

```
Week 1 (Quick Wins):
‚îú‚îÄ LLM-Based Routing (1.5h) ‚úì High impact, low effort
‚îú‚îÄ Memory-Aware Routing (0.75h) ‚úì Easy, helps follow-ups
‚îî‚îÄ Health-Aware Agent Selection (0.5h) ‚úì Quick reliability win

Week 2-3 (Medium Efforts - pick 1):
‚îú‚îÄ Suspension & Resume (1.5d) ‚≠ê Best UX improvement
‚îú‚îÄ Agentic RAG (2d) ‚≠ê Best trustworthiness
‚îî‚îÄ Streaming Optimization (0.5d) ‚≠ê Quick latency win

Week 4+ (Strategic - pick 1-2):
‚îú‚îÄ Structured Routing Supervisor (1-2w) üöÄ Next-level routing
‚îú‚îÄ Continuous Evals (1-2w) üöÄ Quality assurance
‚îî‚îÄ Memory-Optimized Windows (3-4d) üöÄ Scalability
```

---

## üéØ Recommended Starting Point

**Do this first** (3-4 hours, 40% better results):
1. ‚úÖ LLM-Based Routing (1.5h)
2. ‚úÖ Memory-Aware Routing (0.75h)
3. ‚úÖ Detect Multi-Agent Needs (1.75h)

Then pick ONE medium effort:
- **If UX matters**: Suspension & Resume (1.5d) ‚Üí Users see progress
- **If accuracy matters**: Agentic RAG (2d) ‚Üí Responses backed by docs
- **If speed matters**: Streaming Optimization (0.5d) ‚Üí Faster card updates

---

## üîó Code Integration Points

**Files to modify** (in priority order):
1. `lib/agents/manager-agent.ts` (lines 217-625) ‚Üí LLM routing
2. `lib/workflows/manager-routing-workflow.ts` ‚Üí Add memory context
3. `lib/agents/memory-integration.ts` ‚Üí Smart history loading
4. `lib/workflows/okr-analysis-workflow.ts` ‚Üí Add suspend/resume
5. `lib/tools/` ‚Üí Add RAG tools if pursuing Play 2

**Files already built** (leverage):
- ‚úÖ `manager-routing-workflow.ts` ‚Üí Use for confidence-based routing
- ‚úÖ `okr-analysis-workflow.ts` ‚Üí Pattern to follow
- ‚úÖ `observability-config.ts` ‚Üí Already traces everything
- ‚úÖ `memory-mastra.ts` ‚Üí Already has query() method

---

## üí° Why These Matter

**Current state**: You're using Mastra as a "better LangChain" (agents + memory)

**Potential state**: You're using Mastra as an **agent orchestration platform** (workflows + memory + routing + recovery)

The difference:
- Better LangChain: Single agent with tools
- Agent orchestration: Multi-agent system that routes, recovers, and learns

This doc unlocks that second mode.
